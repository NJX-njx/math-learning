在上一个视频中，你看到了 一个导致平方损失的优化示例。 现在， 机器学习中还有另一个非常重要的函数，叫做日志丢失。 在这段视频中，我 还将通过一个例子向你展示这一点。 我喜欢看到 日志丢失的方式与我喜欢 使用掷硬币来查看大多数机器学习和统计数据的方式相同。 我们来玩个游戏吧。我要掷硬币 10 次然后我们要看看结果。 如果结果是七个正面然后是三个反面， 那么你就会赢很多钱。 如果他们不是，那么你就赢不到任何钱。 获胜的机会非常小。 但是，你可以选择 你想使用的硬币，它可能是一个有偏见的硬币。 这是硬币 1。 它降落在头部的概率为 70％，降落在故事中的概率为30％。 Coin 2是一枚公平的硬币， 因此它正面 和反面着陆的概率为50％。 硬币 3 也有偏差，与硬币 1 相反。 这个降落在头部的概率 为30％，故事中的概率为70％。 这是一个测验。 为了最大限度地提高获胜机会，你会选择三枚硬币中的哪一种？ 硬币 1、硬币 2 还是硬币 3？ 请记住，要在这场比赛中获胜， 你必须将硬币落在正面 七次，然后在接下来的三次中落在反面。 为了选出最好的硬币， 让我们计算一下 用三个硬币中的每一个在游戏中获胜的概率。 对于硬币 1，正面着陆的概率 为 0.7，反面着陆的概率为 0.3。 七次落在正面中 是 0.7^7，因为这些概率是独立的事件，因此 会相乘。 对于最后三条尾巴， 则乘以 0.3^3。 因此，0.7^7 乘以 0.3^3 是 0.00222。 这个概率很小， 但让我们计算一下其他概率。 对于硬币 2 来说，七个正面是 0.5^7 乘以 三条尾巴是 0.5^3，那是 0.00097。 最后，对于硬币 3 来说，是 0.3^7 乘以 0.7^3，那是 0.00008。 情况甚至更糟。 如你所见，硬币 1 是最好的。 是帮助我们以 最高概率赢得比赛的那个。 所以这就是你可以选择的那个。 但是，这里有一个问题。 在 你可能创造的所有可能的硬币中，硬币1是最好的硬币吗？ 好吧，为此，我们需要微积分。 让我们选一枚硬币 ，假设正面着陆的概率为 p ，反 向着陆的概率为 1 减去 p，因为它们应该加上 1。 我们要做的是找到p的最优值，这个值 可以帮助我们 最大限度地提高获胜概率。 首先，获胜的概率是多少？ 好吧，将硬币落在正面七次， 概率为 p^7， 将硬币落在尾部三次的概率为 1 减去 p 立方体。 假设 p 的 g 是获胜的概率， 那是 p^7 乘以 1 减去 p 的三个， 我们需要用微积分来最大化 p 的 g 。我们如何最大化 p 的 g？ 好吧，我们可以 取导数并将其设置为零。 这是 g 相对 p 的导数。我们将 计算 p^7 乘以 1 减去 p 立方体的导数。 首先，我们使用乘积法则来 拆分 p^7 和减去 p 的立方体。 我们得到 p^7 乘以 1 减去 p 立方体加上 p^7 乘以 1 减去 p 立方体的导数。 现在，我们将 分别计算其中的每一个。 对于第一个，请记住 导数 p^7 是 7p^6。 对于第二个，你可以在这里使用链式规则。 1 减去 p 立方体的导数 是 3 乘以 1 减去 p 平方， 但随后你需要取内部的导数， 而 1 减去 p 相 对 p 的导数为负 1。 现在我们可以做一些代数来 像这样重组它，然后像这样分解，它 必须等于零。 有三个东西的乘积必须为零。 要使三样东西的乘积为零 ，至少其中一个必须为零。 如果第一个因子为零 p^6，则意味着 p 必须为零。 要使第二个因子为零， 则 1 减去 p 必须为零， 这意味着 p 等于 1。 要使第三个因子为零， 我们需要 p 等于 0.7。 这是最佳点的三个候选点。 现在，第一个有意义吗？ 好吧，如果 p 等于 0， 那么硬币将始终以尾巴着陆 ，你将没有机会在正面中着陆 七次。那个被丢弃了。 出于同样的原因， 第二个也会被丢弃， 因为如果 p 等于 1， 那么硬币总是正面落在正面 并且不可能掉落三次尾巴。 因此，答案是 p 等于 0.7。 事实上，硬币1 是人们可以选择的最好的硬币。 那是一项艰巨的工作。 有更简单的方法来做到这一点吗？ 好吧，接下来我将向你展示 一种更简单的方法。 这实际上有点违反直觉， 因为我们必须采取 一个步骤，让它看起来 更难，然后看起来会更容易。 步骤是 p 的 g 的对数。因为， 如果 p 的 g 是最大值， 那么它就是 p 的 g 的对数。 所以最大化 p 的 g 的对数与最大化 p 的 g 是一样的 。那么 p 的 g 的对数是多少？ 好吧，这是 p^7 乘以 1 减去 p 立方体的对数。 现在对数有一个非常好的属性， 那就是乘积的对数 是对数的总和。 所以这是 p^7 的对数 加上 1 减去 p 立方体的对数。 它还有另一个非常不错的属性，那就是 p^7 的对数是 p 的 7 对数，1 减去 p 的 对数 是 1 减去 p 的 3 对数。 我们称之为 p 的 G。这是一个简化的版本。 这是概率的对数。 我们想最大化 p 的 G。我们该怎么做？ 好吧，现在我们来看导数。 这个导数其实很不错，因为 请记住 p 的对数的导数是 1 比 p。当我们计算 每张召唤的导数时， 我们得到 p 上的 7 乘以 1 加上 1 乘以 1 减去 p 乘以 1，这个 负 1 来自 链式法则，因为它是 1 减去 p 相对于 p 的导数。我们可以将 分母加起来 7 乘以 1 减去 p 减去 3p 除以 p 乘以 1 减去 p， 这应该等于 0。 当我们可以求解等于 0 的值时， 我们就会得到最佳 p 的候选值。现在请注意，我们已经 排除 p 为零或一的可能性。 因此，分母永远不会为零。 将其设置为 0 等于将 顶部设置为 0。 因此，7 乘以 1 减去 p 减去 3p 必须等于 0。 我们可以求解 p，得出 p 等于 0.7。 我们得到的解决方案与以前相同， 只是方法要简单得多。 现在，这种取对数的技巧是 机器学习中非常流行的技巧。 概率的对数非常、非常 、非常常见且非常有用。 事实上，我们真正想要的是 p 的 g 的负数。这 被称为对数损失，它 在机器学习中是一个非常有用的损失函数。在@@ 分类问题中，你会经常看到它。 我们之所以真正取负 p 而不 是 p 的 g，是因为当 p 介于 零和一之间时 p 的对数实际上是一个负数。 我们希望 p 的负 g 是一个正数， 我们没有像这里那样最大化它，而是最小化 p 的负 g。