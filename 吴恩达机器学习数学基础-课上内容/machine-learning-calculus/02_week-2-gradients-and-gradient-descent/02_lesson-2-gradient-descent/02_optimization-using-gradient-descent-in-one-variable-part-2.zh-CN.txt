在上一个视频中，我们学到了 一种近似最小函数的好方法。 但是，在这里，我们将尝试一些更聪明的东西。 假设你在这里，你 想接近最小值， 你会告诉这个问题该怎么做？ 你会告诉它向右走一点点。 如果重点到这里怎么办？ 好吧，你会叫它向左走。 现在，是否有任何关于 该功能的信息可以帮助您快速做出决定？ 好吧，让我们来看看衍生物。 在这里，斜率 是负的，你正在向右移动， 而这里的斜率是 正的，你正在向左移动。 这就是你需要的信息。 实际上，你需要减去斜率。 因为如果你处 在斜率为负的点， 你需要在该点 的坐标上加一点。 而如果你在斜率为正的地方， 则需要稍微减 掉该点的坐标。 简而言之，如果你想要 一个更接近最小值的新点， 那么那应该是旧点减去斜率。 为什么？因为如果你位于新最小值的左边， 则斜率为负。 你正在减去一个负数 然后向右移动。 如果你在最小值的右边， 那么你减去一个正数， 即斜率，然后向左移动。 现在，如果新点是 X_1，旧点是 X_0， 则公式为 X_1 等于 X_0 减去 X_0 的 f 素数，即斜率。 但是，有一个小警告。 想象一下，你 在曲线的一个非常陡峭的部分， 因为函数很陡峭 ，所以导数非常大。 你正在跳得很远。 现在，跳远可能非常混乱。 原因是因为你实际上可能错过 了最低限度，走得很远然后迷路了。 我们希望迈出一小步 ，在前进的道路上更加安全。 我们如何考虑一小步？ 好吧，你只需修改 公式并输入一个小数字， 乘以斜率，例如0.01。 你可以随心所欲地取任何一个小数字 ，这就是所谓的学习率，表示为 Alpha。 选择良好的学习率是一门完整的科学依据。 现在，这个公式还有额外的好处。 想象一下，如果你在这里 远离局部最小值， 并且在曲线的陡峭部分。 你将迈出一大步， 显然不大，因为你不想 错过重点，走得太远，但相对较大。 但是，如果在 更平坦的零件中，你真的接近最小值， 那么你需要迈出一小步。 现在，它包含在 公式中，因为 如果你在曲线的陡峭部分， 导数是B，你正在迈出一大步。 但是，如果你在平坦的土地 上，那么衍生品很 小，你正在迈出一小步。 想象一下，就像在高尔夫比赛中一样， 如果你离洞 很远，你想用力击中洞才能靠近那里。 但是，如果你真的离它很近， 你想精确地击中它。力量不大。 这种方法被称为梯度下降，它 在机器学习中非常有用 ，这就是它如此受欢迎的原因。 它只包括迭代 这个过程。你从 X_0 开始。 从 X_0 中，您可以使用公式获得 X_1 的点。 然后，你现在 只需在公式中插入 X_1 即可获得另一个基于 X_1 的积分 X_2。你可以继续。 你可以迭代很多次、20 次、 100 次、数千次。 这是一个非常快的算法。 因此，它可以 快速迭代数千次，其结果令人惊叹。 它可以让我们非常非常 接近函数的最小值。 这就是它在机器学习中如此广泛使用的原因。简而@@ 言之，这是算法。 你从 x 的函数 f 开始，目标是找出 x 的 f 的最小值。 首先定义学习速率 并选择起点， 然后执行更新步骤， 即 x_k 等于 x_k 减去 1 减去 Alpha 乘以 x_k 处的导数减去 1。 然后重复此步骤，直到 你足够接近真正的最小值。 当 你的步数变化不大时，你可以看出自己已经足够接近真正的最小值了。 现在让我们在这个例子上试一试。 让我们进行几次迭代。 回想一下，这个函数是 e 到 x 的 x 减去对数， e 到 x 的导数是 x 的减一。 让我们选择一个起点 0.05， 我们的学习速率为 0.005。 第一次迭代是我们找到 导数，然后我们 通过学习速率乘以导数来移动。 我们得到一个新点，0.1447， 它更接近最小值。 然后我们再进行一次迭代。 我们找到导数并减去学习速率乘 以导数得到 0.1735。 然后我们可以重复 很多次，直到我们非常接近最小值。 注意一些有趣的地方 ，那就是在这个算法中， 你永远不需要它来求解 e 到 x 减去 1 而不是 x 等于 0。 你永远不需要求解导数零。 只有当你 采取更新步骤时，你才知道导数，然后将其应用于算法中。 这是一个巨大的进步。