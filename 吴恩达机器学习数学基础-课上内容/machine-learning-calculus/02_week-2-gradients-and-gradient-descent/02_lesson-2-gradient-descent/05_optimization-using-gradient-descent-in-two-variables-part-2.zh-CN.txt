再说一遍，在之前的视频中， 你看到了一种不错的方法，可以让 你更接近桑拿房里最冷的地方。 但是，该 算法中有很多随机步骤。 我们将更精确、更 数学地做到这一点，就像我们之前对一个变量所做的那样， 我们将定义发送 两个变量的梯度。它非常相似。 你从初始位置开始， 只是现在初始位置有 两个坐标，x_0 和 y_0。 以前你经常画切线或 导数，但现在我们有两个变量， 所以我们有两个切线， 这个在这里。 请注意，它非常陡峭。 我们将沿着 导数增加的方向在底线中画出那个幅度。 因为它很陡峭，所以它是一个很大的向量。 现在我们要对 渐变的另一个分量做同样的事情。 我们在地板上得到一个较小的向量， 因为第二个渐变 比前一个梯度要平整得多。 现在，让我们计算这两个向量 或具有两个坐标的向量的总 和，这就是梯度。 现在渐变有一个有趣的特点， 就是梯度上升的方向。 如果你想迈出一小步，到达 最热的地方，你可以沿着 渐变的方向迈出一小步。 但是我们不想变热， 我们想变冷，所以我们朝着 负梯度的方向发展。 负梯度的方向 是如果你迈出一步，就会把 你带到最冷的地方 ，你可以在一个小步骤中得到两个。 我们将朝着 负梯度的方向迈出这一步。 这将是一小步，所以我们 再次将其乘以学习率。 我们得到 x_0y_0 减去 学习率乘以梯度，这是我们的新点， x_1，y_1，这是一个更好的点。 请注意，这与 一个变量的梯度下降完全相同， 不同之处在于 我们只是取梯度而不是导数。 现在让我们看看前面的例子是如何工作的， 这是温度的公式。 它是 85 -1/90 x 平方 乘以 x 减去 6 倍 y 平方乘以 y 减去 6。 让我们运行 梯度下降算法的几个步骤。 让我们从 0.50、0.6 开始。 现在我们必须计算梯度， 即相对于x的偏导数和 相对于 y的偏导数放在向量中。 我们 之前已经计算过这些导数，所以现在是这样。 要找到渐变，我们所需要做的 就是将它们组合成一个向量。 现在，让我们将 x 等于 0.5 和 y 等于 0.6 插入这个向量 中，以获得此时的梯度。 现在，只需迈出一步。 你朝着 梯度乘以学习率的方向移动 ，学习率为 0.05。 这给你一个新的分数不再是 0.50，0.6 了。 现在是 0.5057 和 0.6047。 你朝那个方向前进。 现在你离最小点差一点了。 现在让我们再次运行这个算法。 我们计算梯度，然后 朝该渐变的方向移动。 我们添加了一个新观点。 你只需重复很多次即可。 如果你重复很多次， 那么你 很快就会非常接近最低值。 简而言之，梯度上升算法与 我们在一个变量中使用的算法非常相似。 现在我们有了 xy 的函数 f。 目标是找到最低限度。 第一步是定义学习率 Alpha， 然后选择起点 x_0，y_0。 第二步是更新第 k 个位置，方法是将 k 减去第一个位置，减去 学习率乘以 梯度，k 减去第一个位置。 然后第三步说 重复第二步，直到你 足够接近真正的最小值。 像往常一样，你知道真正的最小值何 时出现，因为那是你 移动得非常缓慢或者可能根本没有移动的时候。 现在，就像 一个变量等级的梯度下降一样， 两个或多个变量仍然有同样的缺点。 例如，你想达到这个全球最低值， 但你可能会不小心 在这里进入这些本地最低值。 克服这个问题的方法， 至少很有可能，是从截然不同的地方开始。 请注意，这里我们从 三个不同的地方开始，每个地方都 将我们带到了不同的本地最小值， 其中一个实际上达到了全球最小值。 但是，就像使用一个变量 一样，无法 确定你是否有全局最小值。 但是，如果你跑了很多次， 那么你得到 的最优解决方案可能相当不错。