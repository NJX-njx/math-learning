到目前为止，你学习了如何 使用导数和梯度求解优化问题。 但是，当你尝试通过 分析获得这些问题的精确解决方案时， 你会开始注意到 这些问题很快就会变得非常复杂， 尤其是在更高的维度上。 在本课中，我将向 你展示一种迭代方法，它在最小 化或最大化函数方面非常强大， 尤其是在许多变量中， 该方法称为梯度下降。 但是，让我们从 一个变量的梯度下降开始，以便在 多个变量中逐步提高梯度下降的幅度。 让我们来看下面的函数， x 的 f 等于 e 等于 x 的 x 减去对数。 这是函数的图， 所以它既漂亮又流畅。 问题是，我们能在这里找到最低限度吗？ 最低限度是这里的某个地方。 我们从之前的课程中知道该怎么做。 我们所要做的就是计算 x 的 f 素数并将其设置为零， 然后求解 x。x 的 f 的导数是多少？ 它是 e 到 x 减去 1 而不是 x。 原因是因为 e 到 x 的导数是 e 到 x，对数 的导数是 1 比 x。 我们所要做的就是求解 e 到 x 减去 1 大于 x 等于 0。 但是，这非常具有挑战性，让我来给你看。 在 x 上求解 e 到 x 减去 1 等于 0 等于求解 e 到 x 等于 1 而不是 x。我挑战你试一试。从@@ 分析上讲，这实际上很难做到。 解是 0.5671 等。 这是一个著名的数字， 被称为欧米茄常数。 但是，从 分析上讲，这确实很难解决，这一事实不应阻止我们尝试一些事情。 问题是，还有其他办法吗？ 这是一种方法。 让我们选择一些随机值，比 方说这里的某个地方。 现在，让我们试 一试，向左移动一点， 然后向右移动一点。 现在我们有两个新要点可以尝试。 这两点中哪一个更好？ 好吧，既然我们正在尝试最小化函数，那么 让我们选择右边的那个， 它在函数中的值较小。 现在我们到了这个时刻，让我们重复一遍。 所以我们向 两个方向移动，看看哪个点更小 ，这个点就赢了。 让我们再试一次。 我们双向移动 ，现在发生了一件有趣的事情 ，那就是 两边的两个点都有更高的值。 因此，让我们解决这个问题。 我们说这一点可能不是最低限度， 但它已经足够接近最低限度了。 现在这种方法还不错，但我们实际上可以 对其进行很多改进，使其真正接近最低水平。 让我在下一个视频中给你看。