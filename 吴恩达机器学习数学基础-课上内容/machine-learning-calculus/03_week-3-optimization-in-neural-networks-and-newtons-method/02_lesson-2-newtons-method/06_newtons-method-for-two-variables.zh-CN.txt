本周的最后，让我们来学习如何应用 牛顿法来优化多变量函数。 正如你所猜测的，这里出现了Hessian。 因此，请记住，在本课开始时，我们看到了牛顿法，并 找到了一个表达式，用于从函数中找出最小值或最大值。 对于一个变量，更新方程的表达式取决于 当前值、一导数和二导数。 我们可以很容易地这样重写，不用除以 xk的质数f，而是乘以xk的质数f的倒数。 那么，在二变量中会发生什么呢？ 嗯，在二变量中，你没有一个坐标， 你有两个坐标x和y。 所以，你的第k次迭代是xk，yk，而你的第k + 1次迭代是xk + 1，yk + 1。 那么，你如何从第k次迭代中得到第k + 1次迭代呢？ 好吧，让我们看看上面的表达式。 这是二阶导数，变成了Hessian。 这是一阶导数，变成了梯度， 注意，二阶导数是逆的，因为你要除以它。 好了，这就变成了黑森矩阵的逆矩阵。 你可以看到，这实际上是围绕变量的， 我只需要做一个n个坐标的向量减去一个m乘n的矩阵， 黑森逆乘以一个n个坐标的向量，这就是梯度。 而且我必须非常小心。 这样写也很诱人。 但这实际上行不通。 你不能在矩阵左边乘梯度。 你必须在右边乘。 原因是矩阵是二乘二，而向量是二乘一， 你不能在矩阵左边乘二乘一的向量。 你只能在右边相乘。 因此，我们使用的是两个变量，而顺序是非常关键的。 现在，要实际证明这个公式是很费劲的， 但我们只需照搬它，并注意到它实际上 是对单变量情况的合理概括。 现在你已经知道了多变量的牛顿方法，让我们把它付诸实践。 这里有一个向上凹的函数，我们要试着找出它的最小值。 所以，首先，让我们来看看关于 x 和 y 的两个偏 导数。 然后是相对于 x 和 y 的偏导数，以及相对于 x 和 y 的偏导数。 因此，通过右边的四个表达式就可以得到 Hessian， 而梯度则由中间的两个表达式形成。 换句话说，这里是梯度，这里是Hessian. 所以现在，利用Hessian和梯度， 你将使用牛顿法. 所以让我们从某一点开始，假设点x0，y0等于4，4. 这里是该点的梯度。 所以我们所做的就是在梯度的表达式中， 我们简单地把数字4和4放在一起，现在让我们求Hessian. 所以现在注意到在这个4，4不是函数的0，但是 我们会用牛顿法越来越接近它. 那么第一个点是什么呢？ 好吧，第二次迭代将是4，4， 第一次迭代，减去黑森逆乘以向量。 而这实际上将是2.58，2.62，这就是新的点。 注意，它更接近0， 0，这就是我们要找的根。 现在，让我们再次迭代。 所以我们有了，这是梯度，这是黑森， 第二次迭代是这个点减去黑森逆变换减去梯度。 所以我们得到了 1.59，1.67。 让我们继续重复。 当你多次重复时，你最终会得到实际的零点。 所以我们需要八步才能得到这么接近的结果，看看吧。 第八次迭代是负17的4.15乘以10， 再减去负17的2.05乘以10。 这真是一个非常小的数字。 真的非常非常接近0，0，而0，0实际上是你要找的最佳 点，因为那是函数的根。 所以正如你所看到的，就像单变量的牛顿方法一样， 双变量牛顿方法是一种非常非常快速的方法，能够 非常接近函数的零点。