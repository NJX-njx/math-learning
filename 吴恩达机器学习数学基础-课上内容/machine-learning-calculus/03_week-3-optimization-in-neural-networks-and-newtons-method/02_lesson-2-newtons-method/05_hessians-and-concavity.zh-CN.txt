现在让我告诉你一个 Hessian在优化方面带来的信息。 请记住，对于一个变量的函数， 可能会发生几种情况。 如果二阶导数为正， 那么你就有一个向上凹的函数， 在这种情况下，有一个局部最小值。 如果二阶导数为负， 那么你的函数是 向下凹的，所以我们将有一个局部最大值。 如果二阶导数为零， 则尚无定论。 你不太了解。 许多变量也会发生同样的情况， 只是你必须再跟踪几件事。 在两个或更多变量中，凹向上是什么意思？ 这意味着它看起来像这个函数， 2x 平方加上 3y 平方减去 xy。 从视觉上看，你可以看出 最小值在此处， 对应于 x 等于 0，y 等于 0。 以此类推，用一个变量大小写， 这看起来像一张笑脸。 但是我们可以用数字表示吗？ 好吧，让我们来看看黑森州。 请记住，在一个变量中， 如果我们想知道它是否是向上凹的， 我们会查看二阶导数 并检查它是否为正。 但是现在我们有了矩阵。 我们如何检查矩阵是正数还是负数？ 你实际上是在看特征值。 让我们计算特征值。 如果你回想起线性代数类， 计算它们的方法是看一下 黑森减去 Lambda I 的方程行列式，这就是 这里的矩阵， 行列式是这个，那就是这个二次方程。 当我们找到这个二次曲的根时， 它们是 6.41 和 3.59。 请注意，它们都是阳性的。 对于矩阵，这 等同于是一个正数。 这意味着矩阵是正定的。 因为 这个矩阵的所有特征值都是正数 ，所以函数是向 上凹的，0.00 是最小值。 现在，让我们来看一个非常相似的函数。 这个是负 2 倍的平方减去 3y 平方减去 xy 加上 15。 这个看起来这里的最大点是 00。 现在，我们来看看黑森矩阵。 这是梯度， 这里是 0、0 处的黑森州。 现在，让我们来看看 Hessian 的特征值。 我们要求解和以前一样的方程式。 H 的行列式减去 Lambda I，我们将其分解， 得到的特征值减去 3.49，减去 6.41。 它们都是负数，所以 就像说矩阵是负数一样。 它实际上被称为负定值。 当矩阵的所有特征值均为负值时， 它就会向下凹陷 ，因此 0.00 是最大值。 现在，并非每个矩阵 的所有特征值都为正 或所有特征值均为负， 可能会发生这样的事情：鞍点。 这是函数 2x 平方减去 2y 平方，这是一个鞍点。 它看起来像马鞍或薯片。 在这里，好吧，这个点 既不是最大值也不是最小值。 现在，让我们来看看 Hessian。 这里是梯度 4x 减去 4y。 0,0 处的黑森就是这个矩阵。 4、0、0，减去 4。 这里的特征值是什么？ 如果我们求解 H 的行列式减去 Lambda I 等于 0，则 得出特征值为负 4 和 4。 在这个矩阵中，并非所有的特征值都是 正的，也不是所有的特征值都是负的。 因此，既不是正 定的，也不是负定的。 因此，我们无法得出任何结论。 在这种情况下，我们之所以这样做 ，是因为这个特征值为 负而另一个特征值为正 ，因此 0，0 实际上是一个鞍点；这个点 既不是最小值也不是最大值。 这是摘要。 我们将在 n 个变量中使用单变量函数、双变量函数和函数。 本地最低限额会怎样？ 在一个变量中，你必须有一张快乐的表情。 x 的 F 素素数大于零。 显然，x 的 f 素数 必须为零才能发生这种情况。 在两个变量中，你必须 有一个上抛物面，就像你在本视频 中看到的凹面函数一样。要@@ 做到这一点，两个特征值都 必须为黑森的正数。 在一般情况下，你只需要 有一个包含 n 个特征值的黑森算子， 所有特征值都大于零。 对于局部最大值，同样的， 你在一个变量中有一张悲伤的表情， 所以二阶导数是负数。 对于双变量情况， 你有一个向下的抛物面，就像 你在本视频中看到的另一个函数一样， 并且两个特征值都必须为负。 在一般情况下，你只需要 有一个带有 n 个特征值的黑森算子， 所有特征值都小于零。 对于其他任何事情，我们都需要更多信息。 在一种变量情况下，我们 认为二阶导数为零， 我们不知道它是局部最大值还是最小值。 它可能是一个，也可能没有。 在双变量情况下， 我们有一个鞍点， 其中一个特征值大于 零，一个小于零。 在一般情况下， 你只需要有些是阳性， 有些是阴性，有些是零， 而这些案例尚无定论。 让我再说一遍，如果所有这些都严格为阳性，那么 你就有局部最低限度。 如果所有值都严格为负， 则使用局部最大值。 如果发生任何事情，如果其中一个为零， 或者其中一些是阳性， 其中一些是阴性的，比如这样， 那么你不知道自己有什么， 至少在这个测试中是如此。