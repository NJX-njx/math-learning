你已经看过如何使用 感知器作为线性回归问题。 但是感知器也可以 代表分类问题， 尤其是二元分类问题。 您所要做的就是更改激活功能。 在这段视频中，我将向你展示如何做。 既然你知道如何 用 感知器和梯度下降来求解回归问题，那么 分类实际上非常相似。 它只有一点并发症， 但还不错。 让我们来看一个 你在课堂前面已经见过的例子。 你创造了一个外星文明然后 你开始听说话的方式。 你记下 他们语言中的四个句子，即这四个句子。 他们的语言中只有两个词， 所以很简单。 但是你想要做的是 能够分辨出每句话的心情。 现在，当外星人说出第一句话时， 你可以注意到他们很高兴。 对于第二句话，他们很伤心。 对于第三个，他们很伤心，对于 第四个，他们很高兴。 这个想法是为了能够 从新句子中推断出心情， 他们是开心还是悲伤。 我们将制作一个模型， 根据句子来预测情绪。 现在，既然我们想要数字， 因为模型需要数字而不是单词， 那么我们要做下表 ，记录 单词 aack 的次数和单 词哔哔声的次数。 第一句话有 aack 这个词的三倍，beep 这个词的零倍。 所以是 3、0，因为第二个是 0 、2，因为第三个是 1 、3，第四个是 2、1。 这就是 我们在本课中要解决的情感分析分类问题的数据集。 我们要做的第一件事就是绘制要点。 以下是要点。 如你所见，快乐点往往在 右下角，悲伤点在左上角。 这可能就是模型要告诉我们的。 但是，让我们把它变成感知器。 对于感知器，我们的输入 标记为 x_1 和 x_2，x_1 是 单词 aack 出现的次数，x_2 是蜂鸣声出现的次数。 再说 一遍，这将像 以前一样通过预测函数，包括求和和偏差等。 y hat 的预测将告诉我们 模型认为句子是快乐还是悲伤。 现在， 红色突出显示的中心节点中的内容表示这条线的位置。 这将 根据 特定单词出现的次数对不同的句子进行分类，分别是快乐还是悲伤。 Decenter 是将 右下角的快乐区和左上角的悲伤区域分开来的那个。 让我们更仔细地探索这个中心节点 ，进一步探索整个分类感知器， 它将 与回归感知器非常相似，但稍加一点。 再说一遍，我们将使用权重 W_1 和 W_2，并 确定任何其他特征的重要性。 比如说，aack 这个词实际上与幸福有关， 那么 W_1 就会是一个很大的数字。 如果哔哔声非常无关紧要， 那么 W_2 将是一个很小的数字。 但是，如果哔哔声恰好 与幸福成反比， 比如说这是一个非常悲伤的词， 那么 W_2 将是负权重。 再说一遍，我们将有一个偏差输入 b。 节点内部会发生什么？ 我们要总结一下。 总和将再次是 x_1、W_1 加 x_2、 W_2。 特征乘以添加的权重加上偏差 b。 就像以前一样。 这将生成一个连续的数字 ，该数字将被称为 set。 正如我所说，这是一个连续的数字 ，可以是数字行中的任何地方。 这个数字可以是负一百万， 可能是正7,000， 可能是零， 可能是一个，可以是随心所欲。 但是，这不是我们想要的输出。 这是我们在任何回归中想要的输出， 因为我们希望任何数字都成为价格。 但是现在我们想要一个介于一和零之间的数字， 因为我们想让它分辨出 它是否快乐，那是一个一，或者如果是悲伤，那就是零。 我们如何将整条数字行 转换为数字 1、 0 或者至少介于 1、0 之间的某个数字。 我们将使用所谓的激活函数， 由 z 的字母 sigmoid 表示。 激活函数实际上非常有用，它 会将数字行中的所有数字压缩 到间隔 0、1 中。 现在你仍然可以得到 介于 0、1 之间的数字。没关系。 如果你得到 0.5，可能是 模型不知道句子是快乐还是悲伤。 如果你得到 0.9 表示模型的东西，那 句话就很开心了。 如果你得到 0.01， 那就意味着事物的模型， 句子，太可悲了。 sigmoid 函数 是这个感知器的重要组成部分。 现在，让我 更详细地探讨一下sigmoid函数。 sigmoid 函数由公式 1 加上 z 的 1 加上 e 给出，这将给出一个数字 0 和 1。 在下一个视频 中，我们将更详细地介绍 sigmoid 函数。 但就目前而言，只要知道它是 感知器中非常重要的部分， 左边的求和会通过 sigmoid 函数传递， 这样我们的输出就会介于零到一之间， 这正是我们的数据集所要求的。