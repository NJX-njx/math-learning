因此，既然我们知道了神经网络的样子，我们所要 做的就是找到这些导数，以便在下降中表现出色。 因此，请记住，目标是调整每个突出显示的权重和 偏差，以减少损失函数 L y，y hat。 因此，基本上，我们需要看看这些权重中的每一个是如何影响损失的。 这就是 L 相对 Wij 的偏导数告诉我们的。 这些偏见也是如此。 这些偏见如何影响损失？ 好吧，dL over dbi 可以告诉我们这一点。 现在，权重 W1 和 W2 如何影响这个偏导数的损失， 以及这种偏差如何影响这个偏导数的损失？ 换句话说，这个偏导数告诉我们 每个权重和偏差要朝哪个方向移动以减少损失。 因此，这正是我们要做的。 我们将计算其中的每一个以减少对数损失函数。 因此，让我们简化一下，只看这里的这些。 Del L 胜过了 del W11、W21 和 b1。 让我们再来看看 del L 对 del W1 和 del L 而不 是 del b。所以让我们回想一下，我们计算 红色音符输出的方式首先是设置 1 是这个求和，然后我们用 西格玛得到了 a1，然后我们对 输入 a1 和 a2 乘以权重 W1 和 W2 以及偏差 b 进行求和。然后 我们对这个集合应用 sigmoid 来得到 y 帽子。 然后我们用 y hat 发现了 y，y hat 的日志丢失了。 因此，在 L 和 W11 之间有很多变量。 我们只需要跟踪所有这些，以制定一个庞大的连锁规则。 因此，为了减少这种日志丢失，我们需要 del L 而不是 del y hat。 原因是因为 L 损失取决于 y 帽子。 现在我的情况要看这个。 因此，我们需要2的导数。 Del y 对 del z 不屑一 顾。现在 z 依赖于 a1。 所以我们需要 del z 而不是 del a1，而 a1 依赖于 Z1。 所以我们需要 del a1 而不是 del z1。 现在，z1 依赖于 W11 所以我们需要在 del W11 上删除 z1。 那是一个巨大的连锁反应。 看上去就是这样，我们 想要找到的 del L ver del W11 等于 del z1 胜过 del W11 × del a1 而不是 del z1 × del z 而不是 del a1 × del y hat over del z × del L over del y hat 所有这一切的乘积是一条非常长的链条法则但 这就是我们真正想要的 del L 而不是 del 11 的原因。 现在让我们分别计算其中的每一个。 del L 比 del W11 怎么样？ 好吧，这个很简单，因为这里是 del z1 而不是 del W11。 这些术语中的每一个实际上都非常简单。 这是什么衍生物？ 好吧，它只是 X1，因为 W11 是变量， X1 是随之而来的常量，其他所有内容都是常数。 导数 0 也是如此。 现在让我们转到我们的 del z1 的第 a1 部分。 那也很简单，因为它是一个西格莫德， 方向是西格莫伊德乘以 1-sigmo。 所以这是 a1 × 1-81。 现在 del z 比 del a1 是什么意思？ 再说一遍，这是一个线性方程。 所以 W1 是衍生物，del y hat 比 del z 更胜一筹了。 这又是一个 sigmoid。 所以是 y hat × 1-y 的帽子。 最后，del L over dely hat 是什么。 这是更难的，但我们已经计算过很多次了。 因此，我们对 del W11 的衍生物 del L 是所有这些因素的产物。 这两个抵消了。 所以我们在这里得到这个表达式。 因此，为了找到误差最小的 W11 的最佳值， 我们使用这个公式进行了梯度下降。 那么 Del L 比 W11 有什么好处。 这里就是这个。 所以我们就是这样改变 W11 的。 你可以想象，这就是你更改任何 Wij 的方式， 你只需要在这里更改下标即可。 但我过一会儿就会告诉你怎么做。 但是，为了冗余起见，为了真正解决这个问题，让我们来看看偏见。 因此，为了再次减少这种情况，我们要快一点， 当 del L over del y hat 时。 这取决于设置，这反过来又取决于a1， 而a1又取决于Z1，而Z1反过来又取决于b1。 所以长链法则就是这个， 所有这些导数的乘积等于 del L 而不是 del b1。 因此，让我们分别 快速地计算其中的一个，因为我们之前已经为其中的大多数计算过了。 del l ver del b1 是 del z1 与 del b1 的乘积。 现在很容易了。 现在是 1。 为什么是 1？ 因为 b1 是这里唯一的变量。 所以这也可能是 b1 加上一些常数。 而这个相对于 b1 的导数仅为 1。 其余的，我们已经知道这是信号函数。 这是一个线性函数。 这又是 sigmoid 函数，也是我们之前已经计算过的函数。 所以事情取消了 ，我们更新 b 的方式是为了找到最优值 b1。 然后我们用这个 del L 进行分级下降而不是 已经有计算器的 del b1，仅此而已。 所以让我总结一下，我们更新 W11 的方式是这样的。 对于 W12 来说，情况略有不同。 对于 b1 是这个，然后对于其他三个权重和偏差，就是这样。 因此，简而言之，如果你对 学习率 alpha 进行这些更新，你会得到更好的权重和偏差。 这就是你更新神经网络第一层的方式。 现在让我们更新第二层。 但是这个要容易得多，因为我们必须记录更少的事情。 因此，以下是你如何做到的。 你想最小化 L、y、y 的帽子，所以你想真正减少它。 Del L over del y hat 是你需要的第一个训练，因为 L 取决于你的帽子。 Y 的帽子取决于 z。 Z 取决于 W1。 因此，你需要这条链式规则 来计算del L而不是del W1。 让我们再次将其与之分开计算。 那么 del L 比 del W1 有什么好处？ 它分布在这里，这是一个线性函数。 所以是 a1。 这里的这个是 sigmoid 所以导数 y hat × 1-y hat，而 这里的这个我们已经计算过了。 所以事情取消了，我们得到了一些非常简单的东西。 要找到 W1 的最优值，你使用这个 公式在下降时表现不错，而且由于你已经计算了导数，所以 你明白了，这是 W1 的高更新。 我要这样重写它，把负数， 双负数变成正数。 这就是你更新 W2 的方式，也是你更新偏见的方式。 简而言之，我们已经看到了如何更新每个 神经网络的权重和偏差。 因此，当我们这样做时，我们实际上会得到一组更好的权重和偏差。 这就是你训练神经网络的方式。