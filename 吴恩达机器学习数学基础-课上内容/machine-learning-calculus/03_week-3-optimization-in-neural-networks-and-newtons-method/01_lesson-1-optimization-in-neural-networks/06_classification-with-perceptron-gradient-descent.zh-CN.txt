既然你已经知道了所有 关于sigmoid函数的知识，那么 让我们回到感知器上。 让我们以 Aack、 beep、beep、beep 这句话为例。 这个词曾经有 Aack 这个词，所以 x_1 是一 个，beep 这个词有三次，所以 x_2 是三个。 假设我们正在 训练算法， w_1 的权重为 4.5，w_2 的权重 为 1.5，偏差为 2。 感知器所做的是获取这个输入，将 它们乘以权重函数，然后应用 总和，然后应用 sigmoid函数，以得到预测的y-hat。 这个想法是，我们要将 y-hat 与损失函数一起使用， 它告诉我们 y-hat 离 应有的距离有多远，以便更新权重。 假设 y 为零， 因此句子已设置。 然后我们要用它来更新 w_1、 w_2 和 b。 基本上，我们要测量 y-hat 距离 y 有多远，这就是错误所在。 现在你可能会认为 y-hat 减去 y 效果非常好， y-hat 减去 y 平方， 或者半乘以二分之一减去 y 平方。 我们之前见过的所有其他函数，都起作用了。 但是对于分类而言 ，最有效的方法称为日志丢失。 接下来，我将向您展示 日志损失是如何计算的。 现在，日志丢失将被称为 L of y， y-hat，我们将使用该错误来 更新三个权重并降低错误。 让我们回顾一下。 我们有一个预测函数 y-hat，它激活了 sigmoid，它应用于 感知器发出的求和，我们 有一个损失函数，即对数损失。 现在，日志丢失是你之前已经见过的。 你还记得我们在 上一节中看到的那个例子吗，当时我们试图找到 能够容纳一些数据集的理想硬币？ 这是一枚硬币，你需要翻转 10 次，你需要获得 七次正面和三 次反面，你需要找到最适合的硬币。 完美的硬币是通过 使用对数最小化一些损失函数来获得的。 正是这样。这@@ 实际上是 这个问题的损失函数，你可以看到，如果 y 为零，那么 y-hat 很 小时它很小，y-hat 很大时它很大。 如果 y 为一，则情况恰恰相反。 当 y-hat 接近 1 时，这个函数很小； 当 y-hat 接近零时，这个函数很大。 换句话说， 如果 y 和 y-hat 彼此相距很远，则这个 L of y，y-hat 是一个很大的 数字； 如果它们彼此靠近，则为小数字。 我想为这个功能使用日志丢失的原因有很多。 一是数学运算效果非常好，但 另一个是这个函数 具有概率性，原因 与他用硬币从这个例子中脱颖而出的原因相同。 分类问题具有很高的概率性，因为 您可以将输出 y-hat 视为概率。 假设 y-hat 是 80% 或 0.8， 这意味着模型给 句子开心的概率为 80%。 让我们回到主要目标。 主要目标是找到完美的权重 w_1、 w_2 和 b。 这将使我们获得对 数损失 L (y, y-hat) 误差最少的 y-hat。 为了能够做到这一点， 我们需要回到梯度下降模式。 我们将 更新权重 w_1 的梯度下降公式称为旧 w_1 减去学习速率乘 以 w_1 和 w_2 的对数损失的导数为 w_2 减去学习速率乘以对 数损失乘以 w_2 的导数， b 偏差等于偏差 减去 b 导数的对数损失的 Alpha 导数。 现在要开始算法，我们只需从权重的 任意值开始偏见，我们将开始设计过程。 我们从随机变量开始，然后计算 偏导数，然后进行更新。 在下一个视频中，我将向你展示如何 找到这些偏导数，你 会看到sigmoid和对数损失 实际上非常好地结合在一起， 可以得到一些非常好的导数。