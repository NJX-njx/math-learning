因此，既然你知道如何计算对数损失相对于 神经网络的所有权重和偏差的导数。 我将向你展示如何使用它并对 下降进行分级来训练你的神经网络。 因此，让我们开始构建更大的神经网络， 这个神经网络将分为三层。 一个带有输入 x1 和 x2 的输入层，然后是 第一层，里面有一些感知器和一个偏差， 然后它进入第二层，里面有更多的感知器和偏差。 这将输入到最终的输出 层，y帽子的输出将从该层中出来。 现在我们需要为权重找到一些表示法，所以 让我们为第一层添加一个 1 的上标， 这些上标将输入到求和中，上标 1 和设置 2 上标 1。 因此，上标 1 的所有内容都在第一层中， 现在这些是在 sigmoid 之后获得的 a。 第二层的任何权重和偏差都将 有一个上标 2 以及 通过在集合上应用 sigmoid 获得的集合和 a。 最后，第三层上的任何东西的上标都将是 3， 你可以想象，如果我继续添加图层，我们将继续使用 4、5、6等的上标。 所以这是 神经网络中所有权重的非常一致的表示法，像往常一样，y 的 l of y 是 这个公式给出的对数损失函数，也是我们想要最小化的函数。 因此，现在为了这个网络的目的，让我们 稍微清理一下神经网络，实际上我们只需要担心几个权重。 通过对称性， 其他权重的计算结果都是一样的，所以我们只关心这里的这些权重。 因此，反向传播、 反向传播的目标是我们要用来训练神经网络的方法。 它只包括使用 链式规则计算大量导数，然后使用它们来更新我们的评分，就像以前一样 ，唯一的不同是现在我们必须跟踪更多的变量。 因此，像往常一样，这是 对数丢失，我们必须看看日志损失相对于所有这些权重有何变化 我只在盘旋其中的几个实际上是一大堆。 但是所有东西都可以用这里的这些绿色来计算， 所以我们需要计算什么，我们需要计算DY帽上的DL，以及 所有的 DL 而不是其他任何东西。 因为这告诉我们如何移动每个权重，所以 让我们来看一下神经网络的这一部分，最后的部分。 像往常一样，我们需要建立一条链，因为我们需要 分布式的 DL 而不是 Dw superscript 3。 这将作为这 3 个的乘积获得， 因为 那是命令链，也就是介于 w 上标 3 和 l 之间的变量链。 因此，一旦我们计算出这个变量，我们就可以对偏差做完全相同的事情。 所以这些我们现在知道如何计算了， 让我们再往前走一点，也是一样的。 我们需要通过 Dw superscript 2 计算 DL， 这些都是介于两者之间的变量。 因此，我们需要制定一个长链变量规则， 每个变量都取决于前一个变量。 但是像往常一样，它们要么是 sigmoid 的线性函数，所以它们 都很容易计算，DL over Db superscript 2 也会发生同样的情况。 这只是一个变量链， 我们需要将它们相乘才能得到链式规则。 现在这里已经计算过了，所以 我们不需要再计算了。 因此，有很多变量的重复使用，因此我们可以将它们存储在这里，然后 继续计算新的变量，否则我们将不得不重新计算很多东西。 因此，存储我们已经计算过的值是 件好事，最后对于 Dw 上标的 DW superscript 2，我们 这里只有所有这些乘积可以从之前计算出来。 在我们已经计算 过的导数的存储库中，这里 有一些新的导数，而且像往常一样，这些导数很容易计算，所以基本上这里涉及很多工作，但 你知道基本步骤是什么。 基本步骤是长链法则和单独计算，这些 神经网络的构建方式是，简单神经网络的每个导数 要么是西格莫德函数，要么是线性函数，因此它们很容易计算。 好消息是，如果你是机器学习从业者，你不需要 真正这样做，有一些很棒的软件包可以很好地做到这一点。 但是我的理念是，我总是希望看到它至少仔细完成一次， 然后你可以使用这些软件包并非常快速地完成工作。