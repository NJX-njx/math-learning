现在我们知道问题出在哪里。 我们想找到 具有最佳权重和偏差的预测函数 ，以最大限度地减少损失函数 L y，y hat。 换句话说，我们想要找到 错误最小的最佳模型。 现在，我们如何找到这个最佳值呢？ 我们将使用梯度下降来最小化 L。 我们将使用梯度下降找到最好的 w_1、w_2 和 b。 回想一下，梯度下降公式以这种 方式更新 w_1 是为了最小化 L。 回想一下 Alpha 是学习速率， 然后它就是这样更新 w_2 的，这就是它更新 b 的方式。 换句话说，你有一些 w_1、w_2 和 b， 一些起始值然后 你所需要做的就是计算这 三个梯度然后 你可以更新它们并获得新的 w_1、 w_2 和 b 那是更好的模型，因为它具有较小的损失函数。 如果你这样做很长时间， 你可以想象你最终会得到 相当不错的权重，这些权重 具有非常低的损耗函数，这意味着模型不错。 现在我要告诉你 如何计算这些导数， 会有很多连锁法则。 左边是函数供 参考，右边 是计算偏导数。 首先，dL 比 db 是多少？ 好吧，看看 L， L 取决于变量 y hat 所以我们需要在 dy hat 上做 dL 然后 y hat 取决于 b so dy hat 而不是 db。 这不过是 我们上周学到的连锁法则。 dW_1 上的 dL 也会发生同样的情况。 L 取决于 yhat 所以我们有 dL 而不是 dy hat， y hat 取决于 w_1 所以时间是 dy hat 而不是 dw_1。dL 大于 dw_2，dL 在 dy hat 上也是如此， 因为 L 取决于 y hat t imes dy hat 而不是 dw_2。 现在我们知道要做什么，我们所 要做的就是计算 这四个导数，因为请注意 d L 在 dy 上重复三次，所以 我们需要计算那个一，然后计算 y 相对 w_1、 w_2 和 b 的三个偏导数。 这些导数比 实际插入整个导数容易得多。 它将问题分解为更容易解决的问题。 让我们分别计算其中的每一个。 让我们从 dL 开始吧 dy hat。 这是我们必须 计算的关于 y hat 的导数。 使用链式法则， 这是 平方的1/2倍，其导数是某物， y减去y帽子。 但是我们仍然需要 更多的东西，因为我们必须取 内部相对于 y 帽子的导数，而 y 减去 y 帽子相对 于 y 帽子的导数只是负 1。 我们那边有减一。 一阶导数只是负 y 减去 y hat。 我们可以把它写成负数 y 加上 y hat， 但我们要这样做， 因为它会让以后的数学变得更容易。 现在让我们转到其他三个， 它们实际上要容易得多。d@@ by day day day day 好吧，请注意，这是 b 的常数， 与 b 无关。 实际上，它将是 1，因为它是 剩余的 b 乘以 db 的导数，所以那是 a 1。 那下一个呢？dy hat over dw_1。 好吧，请注意， 这部分相对于w_1来说是一个常数。 这也可能是 w_1x_1 加 7，其导数简单地是 x_1， 这是 w_1 随附的常数。 同样，dy hat over dw_2， 这部分是一个常数， 所以它只是 x_2，w_2 附带的常量。 现在我们有了这些， 让我们把它们重新插上去。 我要把它们放在右边作为参考。 现在让我们记住我们已经计算出的链式规则 d L 大于 dB 是 dL 大于 dy hat， 也就是减去 y 减去 y hat，乘以 dy hat 大于 db，也就 是 1 所以这是一阶导数。 对于第二个，dL 比 dw_1 是 通常的减去 y 减去 y 的帽子，也就是说 dy 比 dy hat t imes dy hat 大于 dw_1，也就是 x_1，也就是 x_1。 对于最后一个，dL 大于 d w_2，这将是通常的 dL 大于 负数 y 减去 y 的帽子 dy hat 而不是 dw_2，也就是 x_2。 这是我们的三种衍生物。 现在让我们回过头来记住我们的主要目标。 回想一下，主要 目标是找出 w_1、 w_2 和 b 的最优值，以尽可能小的误差为我们提供 y 的预测。 我们将使用 上周开发的工具，梯度下降。 回想一下，梯度下降使用 这些更新公式来获得新的 w_1。 你从一些 w_1 开始，然后将其更新为 w_1 减去学习速率乘以损失 的导数，因为我们要相对于 w_1 最小化损失的导数。 现在这是什么？我们已经计算过了。 它将是负 x_1 乘以 y 减去 y 帽子。 再说一遍，对于 w_2 来说， 这是梯度下降步长，我们知道 这个导数我们已经计算出来了，我们得到了这个。 对于 b，b 将更新为 b 减去 Alpha dL 而不是 db，计算结果为负 y 减去 y 减去 y 帽子。 这是梯度下降步骤。 如果我们多次这样做， 我们会得到一些非常好的权重， w_1、w_2 和 b，它们会有一个 非常小的误差，因此是一个非常好的模型。