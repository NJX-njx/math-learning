既然你知道什么是感知器， 那么你距离 了解什么是神经网络只有一步之遥。 神经网络只是一堆感知器，这些感知器分层 组织，在这些层中 ，这些感知的信息 被传递到下一层。 如何 像以前一样使用梯度下降来训练这些神经网络。 只是现在我们有许多衍生品。 我们必须取对 数损失相对于 神经网络的每个权重和偏差的导数。让我来告诉你怎么做。 请记住，在上一课中，你处理 了一个二元分类问题，在这个问题中，你 必须进行情绪分析。 使用此处显示的感知器， 其中 z 是 先前变量的总 和时间乘以权重，然后 Sigmoid set 是激活函数，它可以 让你找到预测 y 帽子。 现在，事实证明这些模型 非常简单，而且只能做得很好。 如果你还记得，这个东西的作用 是，它建立了一个线性边界，在 快乐和悲伤的句子之间划清界限。 但也许语言比这复杂得多。 也许它需要 更复杂的边界，而不仅仅是一条线。 为此，我们 不仅使用一个感知器，还使用 几个感知器组合在一起。 这就是所谓的神经网络。 这里我们有一个感知器， 上面是红色的感知器，然后我们在底部 放了一个绿色的感知器。 现在我们有两种看法 ，它们可能起到不同的作用。 我们如何处理这两个戒律的产出？ 我们把它们插入另一个感知器，即 紫色感知器。 那是一个神经网络。 你可以想象，一个非常 复杂的神经网络，我们有许多感知 器插入其他感知器， 这些感知器的输出被插入 其他感知器中，然后你有很多、很多、很多层。 你将能够解决非常复杂的问题。 现在，我们将用两层来做， 一层是红绿色，另一层是 紫色， 这样我们就可以真正正确地处理数学运算。 这是数学运算。 我们要有权重。 在这里，我们将有权重 w11、w21 和进入红色感知器的偏差 b1， 像往常一样，总和 是 z_1 然后你 应用 Sigmoid z_1 然后你要称之为 a_1。 A_1 是从红色戒律中冒出来的东西。 现在，绿色感知器会产生什么？ 类似的是，w_12 是权重， w_22 的偏差是 b_2。 Z_2 是权重乘以 特征加上偏差的通常总和，当你 对此应用 Sigmoid 时，你会得到 a_2。 A_1 和 a_2 是紫色感知器的输入。 这里的权重是 w_1 和 w_2， 然后 z 以 a_1、w_ 1 加上 a_2、w_2 的形式获得， 然后你获得 z 然后对其应用 Sigmoid 函数来获得 y 帽子。 这是一个深度为 2 的神经网络 ，这里有一个输入层，这里 有一个隐藏层 ，还有一个输出层。 但是正如我所说，你可以想象神经网络有 大量的输入。 但是，缺少一件事，对吧？ 感知器通常有偏差 ，紫色感知器 没有偏差。这没问题。 我们把偏差放在这里 ，我们称之为偏差权重 b。 现在我们有了完整的神经网络。 让我们研究一下 隐藏层中红色节点的内部工作原理。 仅关注这个节点， 我们已经确定 a_1， 它的输出由 z_1 的 Sigmoid 给出，而 z_1 由权重和输入 的通常组合以及偏差给出。 所以 x_1、w_11 加上 x_2、w_21 加上 b1_。 绿色感知器也会发生同样的情况。 A_2 以 z_2 的 Sigmoid 的形式给出，其中 z_2 是权重乘 以特征加上偏差 的通常总和。 现在，让我们来看看紫色感知器。 对于紫色感知器， 输出是 y 帽子，是 z 的 Sigmoid。 Z 又是 a_1 w_1 加 a_2、w_2 加上 b 的总和。 紫色感知器的新输入 是 绿色和红色感知器的输出，因此 a_1 和 a_2，然后乘以权重 w_1 和 w_2，然后 b 被加上。 这就是神经网络的输出。 提醒一下，以下是所有变量。 以下是神经网络的全貌，包括 每个节点背后的内部工作原理和数学运算。 现在，既然我们在处理分类问题，我们就 不要忘记错误函数是我们在 之前的问题中看到的帮助我们测量误差的对数损失，也 就是说 l、y、 y 等于 y 帽子的负对数减去 1 减去 y 帽子的对数， 其中 y hat 是预测，y 是我们想要达到的 训练数据中的目标。 现在，我们知道了神经网络的工作原理， 让我们开始训练它们吧。