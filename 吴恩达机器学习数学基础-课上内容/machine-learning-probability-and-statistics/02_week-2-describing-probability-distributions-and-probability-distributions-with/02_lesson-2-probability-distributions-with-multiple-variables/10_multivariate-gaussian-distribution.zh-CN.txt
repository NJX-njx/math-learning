以前，您学习 了正态分布或高斯分布。 这不是一个变量， 但你可以在 更多变量中使用高斯分布，它被称为多变量高斯。 它看起来非常漂亮，如果你用两个变量来看， 它实际上看起来像一个钟声。 这个也出现在很多地方， 特别是，它在机器学习中经常出现。 让我给你看。回想一下 给定以下 PDF 的一个变量的高斯分布公式。其中@@ 有参数 Mu 是 钟的中心，Sigma 是钟声的蔓延。 现在，有一个问题， 如果你有多个变量会怎样？ 假设你有变量 X_1， X_2 到 x_n。 假设我们有两个变量，例如， H 是成年人的身高（以英寸为单位）， W 是成年人的体重（以磅为单位）。 如果你有一个包含 1,000 个点的数据集，其中包含每个人的身高和体重， 那么你可以查看 边距，注意到它们 都是正态分布或高斯分布， 有一些平均值和一些标准差。 现在，加入的发行版是什么样子？ 如果这两个变量是独立的， 那么联合 PDF 将是 边距 PDF 的产物 ，稍微重新排列一下术语， 你就会得到这个表达式。 但是现在，当你 计划数据集的密度时，这不是你所看到的， 两个分布都是钟形的， 但是在独立的情况下， 分布是完全对称的。 在因变情况下， 您会看到分布 沿着一条斜率为正的线拉长。 从上面看，就是这样。 这些是分布的水平曲线。 因此，绿色曲线向 上，紫色曲线向下。 这就像从山顶看到一座山。 是什么原因导致了 关节分布的变形？ 好吧，事实证明这是 两个变量之间的协方差。 它们是身高和体重， 人们在身高较高时体重往往会更大， 所以这就是正相关性的用武之地。 在左边，你会有两个自变量 ，因此你有这些圆圈。 让我们做一些代数操作， 看看能否把它改得更好一点。由@@ 于 H 和 W 是高斯的 ，因此密度的乘积如下所示， 其中指数是两个高斯指数的和。 这个平方和实际上可以看作 是这里这个向量的平方范数，它与 转置向量与其自身之间的点积相同。 现在，向量 h 减去 mu_w 减去 Mu， 可以改写为两个向量 h、 w 和 mu_H w 的减法。 为了使向量的每个元素 乘以不同的常数， 你需要在两者之间放一个对角矩阵。 这是对角矩阵。 这最终可以写成 差异转置 乘以对角线矩阵的逆矩阵与对角线 每个元素的方差乘以差异向量的点积。 这里的这个矩阵就是协方差矩阵。 请注意，由于变量是独立的， 因此必须是对角线。 向量 mu_h，mu_w 是向量均值 ，我们将用粗体 Mu 表示。 乘积 Sigma_H， Sigma_W 只是协方差矩阵行列式 的平方根。 将所有内容结合在一起，你可以 在这里得到这样的表情。 请记住，只有当 W 和 H 独立时，这才有效。 用 Mu 和 Sigma 替换， 你可以这样重写这个表达式。当@@ 变量相 因时，实际情况下会发生什么变化？ 好吧，当然， 联合分配是边际的产物，这已不再是事实。 但是，从属案例和独立案例 都有钟形表面。 这个通用表达式实际上 在因变情况下也是有效的， 唯一的区别是 协方差矩阵不再是对角线的， 现在非对角线量对 应于变量之间的协方差。 既然你熟悉 单变量高斯的公式，那么 让我们看看 多变量高斯的公式是什么 样子，只需将其与你已经知道的相反即可。 x 的 F 之所以变成这样， 是因为现在你要处理更多的变量。 考虑到变量的数量，我们仍然会使用这个常数 1，而不是除以 2Pi 的平方根，而是除以 2pi^n/2。 现在对于单变量分布， 你有显示 变异的标准差，但是对于多线， 我们将使用协方差矩阵， 因为它告诉我们曲线中的变异。 更具体地说，我们使用协 方差矩阵的行列式， 该矩阵捕获价差的体积。 行列式越大， 点差越大。 接下来，让我们看一下指数项。 因此 x 是一个随机向量， Mu 捕获 空间中每个变量的总体均值。协@@ 方差矩阵的逆矩阵适用于对方差和协方 差进行标准化和重新缩放。 要@@ 知道，由于你处理的是 多变量分布，所以 所有的标量值都被向量所取代， 标量方差会被协方差矩阵所取代