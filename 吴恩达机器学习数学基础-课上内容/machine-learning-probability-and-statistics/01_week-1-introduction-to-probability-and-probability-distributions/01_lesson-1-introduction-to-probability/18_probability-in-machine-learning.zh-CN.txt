所以你可能想知道为什么我们这么多地谈论概率， 它们与机器学习有什么关系？ 好吧，事实证明，机器学习在很大程度上与概率有关。 很多时候，在机器学习中，你想要做的是，考虑到 其他一些因素，你想计算某件事的概率。 因此，例如，在垃圾邮件检测中，您会尝试 根据电子邮件或 收件人中的文字、电子邮件的附件或其他功能来计算电子邮件成为垃圾邮件的概率。 这是一种有条件的概率，因为考虑到某些特征，垃圾邮件的概率是存在的。 另一个例子是情感分析，你想确定一段 文字是快乐还是悲伤。 在这种情况下，你想找出 给定这段文字所包含的单词后感到满意的概率。 让我们再举一个例子，图像识别。 在这里，你可以尝试找出图像是否有特定的东西。 假设你想识别图像中是否有猫。 因此，你可以 根据图像中的像素来计算图像中有猫的概率。 因此，这些都是条件概率。 但是，纯概率在机器学习中也经常出现。 机器学习的另一个重要领域叫做生成式机器 学习，它是无监督机器学习的一部分，你想 最大限度地提高概率。 因此，例如，图像生成，如果你看过这些 由计算机生成的精美人脸图像，那么在这里你想最大限度地提高 一堆像素形成人脸的可能性。 或者在文本生成中，你想最大限度地提高 一堆单词是敏感的文本并且它谈论某件事的可能性。 因此，所有这些都是使用大量概率的机器学习的示例， 现在让我详细说明一下。 因此，在之前的视频中，你可以看到基本定理的实际应用， 首先你会发现电子邮件是垃圾邮件的概率，但 只是初始概率， 即用垃圾邮件的数量除以电子邮件总数。 然后有一个事件，例如，电子邮件中包含彩票这个词，然后 是一个后缀词，它通过创建可能性树来完善这种可能性。 这给了我们四种可能性，那就是电子邮件是垃圾 邮件和彩票，是垃圾邮件而不是彩票，是火腿和彩票 ，是火腿没有彩票。 然后，您 通过忘记所有不包含彩票一词的电子邮件并在 那里进行计算来进一步计算垃圾邮件和彩票的可能性。 然后，给定彩票的垃圾邮件概率等于垃圾邮件和 彩票的概率除以垃圾邮件和彩票以及 火腿和彩票的概率之和，这就是后数。 但是，当你从高层次上看这个 问题时，真正发生的事情是，你通过某种东西创建了一个机器学习分类器，它可以计算出一件事物给定另一件事的概率。 在很多情况下，这就是机器学习的本质。 想象一下图像识别。 假设图像识别分类器会告诉你图像是否 包含猫。 这实际上是 根据某些事件告诉你图像中有一只猫的可能性，而事件就是图像中的像素。 因此，分类器会告诉你给定像素一、像 素二一直到像素 n 的概率。 这是一个图像分类器。 另一个例子是在医疗领域。 假设你有很多患者的人口统计 和症状以及很多指标，你想知道患者是否健康。 因此，你要做的就是 根据患者的症状和病史来计算他们健康的概率。 因此，您可以构建一个计算此条件概率的模型。 因此，这里再举一个例子，情绪分析。 在情感分析中，你要做的就是训练一个模型，告诉 你给定的句子是快乐还是悲伤。 你在这里要做的是 根据某些事件计算电子邮件满意的概率，即有条件的概率。 事件就是句子中的单词。 因此，让我们考虑图像识别问题，看看它是如何工作的。 你要做的是训练一些模型，这就是模型，而模型所 做的就是拍摄一张图像，所以一堆像素，然后 告诉你在给定像素的情况下图像有猫的可能性。 在这种情况下，它将是 0.9，因为它是一只猫。 而且，如果你给我们一些不同的东西，比如一辆车，那么它会说 ，好吧， 考虑到像素，你在这里有一只猫的可能性会很小。 因此，假设为 0.1。 所以你确定这不是一只猫。 因此，正如你所看到的，机器学习就是要找到条件概率。 特别是，这是监督式机器学习，因为你是在 回答有关数据的问题。 例如，图像中是否包含猫？ 这句话开心吗？ 这是垃圾邮件吗？ 等等。 条件概率的另一个非常有趣的例子是生成 模型，例如人脸生成模型。 这个想法是训练一个模型，该模型可以生成一组像素，从而生成 看起来像人脸的图像。 这是通过尝试在 给定生成的像素的情况下实现图像成为人脸的高概率来实现的。 例如，右边的图像不是真实人物。 它是由一个名为 StyleGan 的模型生成的。 我不认识你，但这真的欺骗了我。 那张脸看起来很真实。