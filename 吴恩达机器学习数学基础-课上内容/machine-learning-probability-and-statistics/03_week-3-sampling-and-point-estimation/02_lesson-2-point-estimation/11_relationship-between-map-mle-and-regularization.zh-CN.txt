在本课中，你学到了几种不同的东西。 您学习了最大似然估计和最大 后验估计， 还学习了机器学习中的正则化。 现在，在这段视频中，我 将向您展示它们是如何组合的。 在这段视频中，我将向你展示正则 化和最大可能性实际上是如何匹配的。 假设你有数据，你 有三个模型可以拟合 这些数据，每个模型都以一 定的概率生成数据，请记 住，模型 只需在线 上的每个点创建一个高斯 并生成 靠近直线或曲线的点即可生成我们之前看到的数据。 第一个生成数据，其 概率为给定模型 1 的数据的一定概率 P。 第二个生成 概率更高的数据，第三个 生成概率最高的数据，因为它是最佳拟合， 所以我们要选择这个。 但是就像以前一样 ，在爆米花投掷比赛和电影中， 我们还必须考虑 模型被选中的可能性。 现在，模型 越简单，被 选中的可能性就越大，我将在一分钟后向你展示。 但是这个想法是，像 模型1这样的简单模型很可能会出现。 像模型 2 这样稍微复杂 一点的模型不太可能出现，像模型 3 这样非常复杂的模型不太可能出现。 我们要做的是，将 这些概率乘以现在的 赢家，而不是第三个。 现在也许赢家是第二个 ，所以这就是获胜的模型。 现在让我向你展示最大可能性和 正则化回归到底是如何配合的。 现在，在回归中 ，你会有一些损失，例如平方损失， 而且你将最大限度地提高使用 模型创建数据的概率。 现在，如果我们在这里加上基数， 那么我们必须 在这里乘以模型的概率。 现在，如果我们进行正则化和回归， 那么除此之外，我们还会添加一个正则化项。 现在我们如何将左向右转？ 那么，将产品转化为金额的好方法是什么？ 我们采用对数，这就是我们将 左边带有概率的参数转换为 右边带有 损失和正则化项的参数的方式。 但是，这里的房间里有一头大象。 我还没有告诉你我所说 的模型概率是什么意思 ，所以这就是我接下来要告诉你的内容。 我所说的概率，或者更确切地说， 模型的可能性是什么意思？ 假设我们有模型 1、模型 2 和模型 3，模型 1 的概率很高，模型 2 的概率很小 ，模型 3 的概率非常小。 好吧，假设这些是 模型的方程。 我们要做的是从 标准正态分布中选出这些系数， 因此 a_1、a_2 以及所有不超过 a_10 的系数 都将 从标准正态分布中选择。 例如，a_i 的可能性有多大？ 好吧，它将超过 2Pi 乘以 e^-1/2 a_i^2 的平方根。 因此，该 模型的可能性正是这个数字。 现在第二种模型的可能性有多大？ 好吧，我们需要选择 这两个系数，这样就是这个乘积， 第十个模型的可能性将是 所有这些数字的乘积，所有可能 性的乘积。 现在让我们回过头来尝试拟合最佳模型。 如果这些是我们的观点， 那么假设这是一个合适的模型。 给定模型的*P，我们想最大化 Theta 的 P。它们是什么？ 好吧，我们之前确定，如果 这些距离是 d_1 到 d_5， 那么给定模型的 Theta 的 P 将是这里的这个乘积。 模型的 P 是多少？ 好吧，模型的方程是这样的， 那么模型的 P 是 1，比平方根 2Pi e^-1/2 a_1^2 乘以同样的东西 a_2^2。 现在，我们想最大限度 地利用这两个常量的乘积，而且常量很多， 所以我们并不真正需要这些常量。 我们可以除以常数然后将其 余部分最大化，让我们像以前一样取一个对数。 我们在这里取对数。 我们得到的是 i^2 总和的 -1/2 倍。 我们在这里取一个对数， 我们得到-1/2*a_1^2+a-2^2，乘 积变成对数下的总和，现在我们必须将其最大化。 我们可以乘以 -1/2，现在这意味着我们必须 最小化距离的平方和，即 平方损失加上正则化 项，所以这就是正则化项的用 武之地。 因此，当我们尝试从模型 1、模型 2 和模型 3 中挑选模型 时，最大化模型的概率与 最小化 系数的平方和相同，最大化 给定 模型的数据的条件概率与最小化平方损失相同。 新的损失将是 两者的总和，这就是 使用 基于贝叶斯方法的正则化训练模型的方式。